{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lobof\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn import utils\n",
    "from mrcnn import model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn import visual\n",
    "\n",
    "# Import COCO config\n",
    "sys.path.append(os.path.join(ROOT_DIR, \"coco_model/coco/\"))  # To find local version\n",
    "import coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Python34\\Mask_RCNN\\mask_rcnn_coco.h5\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "print(COCO_MODEL_PATH)\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def random_colors(N, bright=True):\n",
    "    \"\"\"\n",
    "    Generate random colors.\n",
    "    To get visually distinct colors, generate them in HSV space then\n",
    "    convert to RGB.\n",
    "    \"\"\"\n",
    "    brightness = 1.0 if bright else 0.7\n",
    "    hsv = [(i / N, 1, brightness) for i in range(N)]\n",
    "    colors = list(map(lambda c: colorsys.hsv_to_rgb(*c), hsv))\n",
    "    random.shuffle(colors)\n",
    "    return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_mask(image, mask, color, alpha=0.5):\n",
    "    \"\"\"Apply the given mask to the image.\n",
    "    \"\"\"\n",
    "    for c in range(3):\n",
    "        image[:, :, c] = np.where(mask == 1,\n",
    "                                  image[:, :, c] *\n",
    "                                  (1 - alpha) + alpha * color[c] * 255,\n",
    "                                  image[:, :, c])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_instances(image, boxes, masks, class_ids, class_names,\n",
    "                      scores=None, title=\"\",\n",
    "                      figsize=(16, 16), ax=None,\n",
    "                      show_mask=True, show_bbox=True,\n",
    "                      colors=None, captions=None):\n",
    "    \"\"\"\n",
    "    boxes: [num_instance, (y1, x1, y2, x2, class_id)] in image coordinates.\n",
    "    masks: [height, width, num_instances]\n",
    "    class_ids: [num_instances]\n",
    "    class_names: list of class names of the dataset\n",
    "    scores: (optional) confidence scores for each box\n",
    "    title: (optional) Figure title\n",
    "    show_mask, show_bbox: To show masks and bounding boxes or not\n",
    "    figsize: (optional) the size of the image\n",
    "    colors: (optional) An array or colors to use with each object\n",
    "    captions: (optional) A list of strings to use as captions for each object\n",
    "    \"\"\"\n",
    "    # Number of instances\n",
    "    N = boxes.shape[0]\n",
    "    if not N:\n",
    "        print(\"\\n*** No instances to display *** \\n\")\n",
    "    else:\n",
    "        assert boxes.shape[0] == masks.shape[-1] == class_ids.shape[0]\n",
    "\n",
    "    # If no axis is passed, create one and automatically call show()\n",
    "    auto_show = False\n",
    "    if not ax:\n",
    "        _, ax = plt.subplots(1, figsize=figsize)\n",
    "        auto_show = True\n",
    "\n",
    "    # Generate random colors\n",
    "    colors = colors or random_colors(N)\n",
    "\n",
    "    # Show area outside image boundaries.\n",
    "    height, width = image.shape[:2]\n",
    "    ax.set_ylim(height + 10, -10)\n",
    "    ax.set_xlim(-10, width + 10)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    masked_image = image.astype(np.uint32).copy()\n",
    "    for i in range(N):\n",
    "        color = colors[i]\n",
    "\n",
    "        # Bounding box\n",
    "        if not np.any(boxes[i]):\n",
    "            # Skip this instance. Has no bbox. Likely lost in image cropping.\n",
    "            continue\n",
    "        y1, x1, y2, x2 = boxes[i]\n",
    "        if show_bbox:\n",
    "            p = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2,\n",
    "                                alpha=0.7, linestyle=\"dashed\",\n",
    "                                edgecolor=color, facecolor='none')\n",
    "            ax.add_patch(p)\n",
    "\n",
    "        # Label\n",
    "        if not captions:\n",
    "            class_id = class_ids[i]\n",
    "            score = scores[i] if scores is not None else None\n",
    "            label = class_names[class_id]\n",
    "            x = random.randint(x1, (x1 + x2) // 2)\n",
    "            caption = \"{} {:.3f}\".format(label, score) if score else label\n",
    "        else:\n",
    "            caption = captions[i]\n",
    "        ax.text(x1, y1 + 8, caption,\n",
    "                color='w', size=11, backgroundcolor=\"none\")\n",
    "\n",
    "        # Mask\n",
    "        mask = masks[:, :, i]\n",
    "        if show_mask:\n",
    "            masked_image = apply_mask(masked_image, mask, color)\n",
    "\n",
    "        # Mask Polygon\n",
    "        # Pad to ensure proper polygons for masks that touch image edges.\n",
    "        padded_mask = np.zeros(\n",
    "            (mask.shape[0] + 2, mask.shape[1] + 2), dtype=np.uint8)\n",
    "        padded_mask[1:-1, 1:-1] = mask\n",
    "        contours = find_contours(padded_mask, 0.5)\n",
    "        for verts in contours:\n",
    "            # Subtract the padding and flip (y, x) to (x, y)\n",
    "            verts = np.fliplr(verts) - 1\n",
    "            p = Polygon(verts, facecolor=\"none\", edgecolor=color)\n",
    "            ax.add_patch(p)\n",
    "    ax.imshow(masked_image.astype(np.uint8))\n",
    "    if auto_show:\n",
    "        plt.show()\n",
    "        \n",
    "    return masked_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_rois(image, rois, refined_rois, mask, class_ids, class_names, limit=10):\n",
    "    \"\"\"\n",
    "    anchors: [n, (y1, x1, y2, x2)] list of anchors in image coordinates.\n",
    "    proposals: [n, 4] the same anchors but refined to fit objects better.\n",
    "    \"\"\"\n",
    "    masked_image = image.copy()\n",
    "\n",
    "    # Pick random anchors in case there are too many.\n",
    "    ids = np.arange(rois.shape[0], dtype=np.int32)\n",
    "    ids = np.random.choice(\n",
    "        ids, limit, replace=False) if ids.shape[0] > limit else ids\n",
    "\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 12))\n",
    "    if rois.shape[0] > limit:\n",
    "        plt.title(\"Showing {} random ROIs out of {}\".format(\n",
    "            len(ids), rois.shape[0]))\n",
    "    else:\n",
    "        plt.title(\"{} ROIs\".format(len(ids)))\n",
    "\n",
    "    # Show area outside image boundaries.\n",
    "    ax.set_ylim(image.shape[0] + 20, -20)\n",
    "    ax.set_xlim(-50, image.shape[1] + 20)\n",
    "    ax.axis('off')\n",
    "\n",
    "    for i, id in enumerate(ids):\n",
    "        color = np.random.rand(3)\n",
    "        class_id = class_ids[id]\n",
    "        # ROI\n",
    "        y1, x1, y2, x2 = rois[id]\n",
    "        p = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2,\n",
    "                              edgecolor=color if class_id else \"gray\",\n",
    "                              facecolor='none', linestyle=\"dashed\")\n",
    "        ax.add_patch(p)\n",
    "        # Refined ROI\n",
    "        if class_id:\n",
    "            ry1, rx1, ry2, rx2 = refined_rois[id]\n",
    "            p = patches.Rectangle((rx1, ry1), rx2 - rx1, ry2 - ry1, linewidth=2,\n",
    "                                  edgecolor=color, facecolor='none')\n",
    "            ax.add_patch(p)\n",
    "            # Connect the top-left corners of the anchor and proposal for easy visualization\n",
    "            ax.add_line(lines.Line2D([x1, rx1], [y1, ry1], color=color))\n",
    "\n",
    "            # Label\n",
    "            label = class_names[class_id]\n",
    "            ax.text(rx1, ry1 + 8, \"{}\".format(label),\n",
    "                    color='w', size=11, backgroundcolor=\"none\")\n",
    "\n",
    "            # Mask\n",
    "            m = utils.unmold_mask(mask[id], rois[id]\n",
    "                                  [:4].astype(np.int32), image.shape)\n",
    "            masked_image = apply_mask(masked_image, m, color)\n",
    "\n",
    "    ax.imshow(masked_image)\n",
    "\n",
    "    # Print stats\n",
    "    print(\"Positive ROIs: \", class_ids[class_ids > 0].shape[0])\n",
    "    print(\"Negative ROIs: \", class_ids[class_ids == 0].shape[0])\n",
    "    print(\"Positive Ratio: {:.2f}\".format(\n",
    "        class_ids[class_ids > 0].shape[0] / class_ids.shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "def draw_box(image, box, color):\n",
    "    \"\"\"Draw 3-pixel width bounding boxes on the given image array.\n",
    "    color: list of 3 int values for RGB.\n",
    "    \"\"\"\n",
    "    y1, x1, y2, x2 = box\n",
    "    image[y1:y1 + 2, x1:x2] = color\n",
    "    image[y2:y2 + 2, x1:x2] = color\n",
    "    image[y1:y2, x1:x1 + 2] = color\n",
    "    image[y1:y2, x2:x2 + 2] = color\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "frame_number = 0\n",
    "\n",
    "import coco\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir is C:\\Python34\\Mask_RCNN\n"
     ]
    }
   ],
   "source": [
    "config = coco.CocoConfig()\n",
    "COCO_DIR = ROOT_DIR  # TODO: enter value here\n",
    "print(\"Dir is\",COCO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_META_SIZE                93\n",
      "IMAGE_MIN_DIM                  800\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           coco\n",
      "NUM_CLASSES                    81\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                20\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(coco.CocoConfig):\n",
    "        GPU_COUNT = 1\n",
    "        IMAGES_PER_GPU = 1\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model object in inference mode.\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True)\n",
    "class_names = [\n",
    "        'BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
    "        'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "        'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',\n",
    "        'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',\n",
    "        'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
    "        'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "        'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
    "        'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
    "        'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "        'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "        'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n",
    "        'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
    "        'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
    "        'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
    "        'teddy bear', 'hair drier', 'toothbrush'\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights  C:\\Python34\\Mask_RCNN\\mask_rcnn_coco.h5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Device to load the neural network on.\n",
    "# Useful if you're training a model on the same \n",
    "# machine, in which case use CPU and leave the\n",
    "# GPU for training.\n",
    "DEVICE = \"/cpu:0\"  # /cpu:0 or /gpu:0\n",
    "\n",
    "# Inspect the model in training or inference modes\n",
    "# values: 'inference' or 'training'\n",
    "# TODO: code for 'training' test mode not ready yet\n",
    "TEST_MODE = \"inference\"\n",
    "# Create model in inference mode\n",
    "with tf.device(DEVICE):\n",
    "    model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR,\n",
    "                              config=config)\n",
    "\n",
    "# Set weights file path\n",
    "if config.NAME == \"shapes\":\n",
    "    weights_path = SHAPES_MODEL_PATH\n",
    "elif config.NAME == \"coco\":\n",
    "    weights_path = COCO_MODEL_PATH\n",
    "# Or, uncomment to load the last model you trained\n",
    "# weights_path = model.find_last()[1]\n",
    "\n",
    "# Load weights\n",
    "print(\"Loading weights \", weights_path)\n",
    "model.load_weights(weights_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=16):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Adjust the size attribute to control how big to render images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=5.74s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "if config.NAME == \"coco\":\n",
    "    dataset = coco.CocoDataset()\n",
    "    dataset.load_coco(COCO_DIR, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Count: 40137\n",
      "Class Count: 81\n",
      "  0. BG                                                \n",
      "  1. person                                            \n",
      "  2. bicycle                                           \n",
      "  3. car                                               \n",
      "  4. motorcycle                                        \n",
      "  5. airplane                                          \n",
      "  6. bus                                               \n",
      "  7. train                                             \n",
      "  8. truck                                             \n",
      "  9. boat                                              \n",
      " 10. traffic light                                     \n",
      " 11. fire hydrant                                      \n",
      " 12. stop sign                                         \n",
      " 13. parking meter                                     \n",
      " 14. bench                                             \n",
      " 15. bird                                              \n",
      " 16. cat                                               \n",
      " 17. dog                                               \n",
      " 18. horse                                             \n",
      " 19. sheep                                             \n",
      " 20. cow                                               \n",
      " 21. elephant                                          \n",
      " 22. bear                                              \n",
      " 23. zebra                                             \n",
      " 24. giraffe                                           \n",
      " 25. backpack                                          \n",
      " 26. umbrella                                          \n",
      " 27. handbag                                           \n",
      " 28. tie                                               \n",
      " 29. suitcase                                          \n",
      " 30. frisbee                                           \n",
      " 31. skis                                              \n",
      " 32. snowboard                                         \n",
      " 33. sports ball                                       \n",
      " 34. kite                                              \n",
      " 35. baseball bat                                      \n",
      " 36. baseball glove                                    \n",
      " 37. skateboard                                        \n",
      " 38. surfboard                                         \n",
      " 39. tennis racket                                     \n",
      " 40. bottle                                            \n",
      " 41. wine glass                                        \n",
      " 42. cup                                               \n",
      " 43. fork                                              \n",
      " 44. knife                                             \n",
      " 45. spoon                                             \n",
      " 46. bowl                                              \n",
      " 47. banana                                            \n",
      " 48. apple                                             \n",
      " 49. sandwich                                          \n",
      " 50. orange                                            \n",
      " 51. broccoli                                          \n",
      " 52. carrot                                            \n",
      " 53. hot dog                                           \n",
      " 54. pizza                                             \n",
      " 55. donut                                             \n",
      " 56. cake                                              \n",
      " 57. chair                                             \n",
      " 58. couch                                             \n",
      " 59. potted plant                                      \n",
      " 60. bed                                               \n",
      " 61. dining table                                      \n",
      " 62. toilet                                            \n",
      " 63. tv                                                \n",
      " 64. laptop                                            \n",
      " 65. mouse                                             \n",
      " 66. remote                                            \n",
      " 67. keyboard                                          \n",
      " 68. cell phone                                        \n",
      " 69. microwave                                         \n",
      " 70. oven                                              \n",
      " 71. toaster                                           \n",
      " 72. sink                                              \n",
      " 73. refrigerator                                      \n",
      " 74. book                                              \n",
      " 75. clock                                             \n",
      " 76. vase                                              \n",
      " 77. scissors                                          \n",
      " 78. teddy bear                                        \n",
      " 79. hair drier                                        \n",
      " 80. toothbrush                                        \n"
     ]
    }
   ],
   "source": [
    "dataset.prepare()\n",
    "\n",
    "print(\"Image Count: {}\".format(len(dataset.image_ids)))\n",
    "print(\"Class Count: {}\".format(dataset.num_classes))\n",
    "\n",
    "for i, info in enumerate(dataset.class_info):\n",
    "  print(\"{:3}. {:50}\".format(i, info['name']))\n",
    "\n",
    "#print(\"Images: {}\\nClasses: {}\".format(len(dataset.image_ids), dataset.class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'dtype'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-903bac9c44eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcapture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mframe_number\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         frame = display_instances(\n",
      "\u001b[1;32mC:\\Python34\\Mask_RCNN\\mrcnn\\model.py\u001b[0m in \u001b[0;36mdetect\u001b[1;34m(self, images, verbose)\u001b[0m\n\u001b[0;32m   2452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2453\u001b[0m         \u001b[1;31m# Mold inputs to format expected by the neural network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2454\u001b[1;33m         \u001b[0mmolded_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_metas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmold_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2456\u001b[0m         \u001b[1;31m# Validate image sizes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python34\\Mask_RCNN\\mrcnn\\model.py\u001b[0m in \u001b[0;36mmold_inputs\u001b[1;34m(self, images)\u001b[0m\n\u001b[0;32m   2350\u001b[0m                 \u001b[0mmin_scale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIMAGE_MIN_SCALE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2351\u001b[0m                 \u001b[0mmax_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIMAGE_MAX_DIM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2352\u001b[1;33m                 mode=self.config.IMAGE_RESIZE_MODE)\n\u001b[0m\u001b[0;32m   2353\u001b[0m             \u001b[0mmolded_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmold_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmolded_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2354\u001b[0m             \u001b[1;31m# Build image_meta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python34\\Mask_RCNN\\mrcnn\\utils.py\u001b[0m in \u001b[0;36mresize_image\u001b[1;34m(image, min_dim, max_dim, min_scale, mode)\u001b[0m\n\u001b[0;32m    423\u001b[0m     \"\"\"\n\u001b[0;32m    424\u001b[0m     \u001b[1;31m# Keep track of image dtype and return results in the same dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m     \u001b[0mimage_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m     \u001b[1;31m# Default window (y1, x1, y2, x2) and default scale == 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'dtype'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "#fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "#out = cv2.VideoWriter('output_catpure.avi',fourcc, 20.0, (640,480))\n",
    "\n",
    "    # these 2 lines can be removed if you dont have a 1080p camera.\n",
    "    #capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "    #capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "while True:\n",
    "        ret, frame = capture.read()\n",
    "        frame_number += 1\n",
    "        results = model.detect([frame], verbose=0)\n",
    "        r = results[0]\n",
    "        frame = display_instances(\n",
    "            frame, r['rois'], r['masks'], r['class_ids'], dataset.class_names, r['scores']\n",
    "        )\n",
    "        #cv2.imshow('frame', frame)\n",
    "        # Write the resulting image to the output video file\n",
    "        print(\"Writing frame {} / {}\".format(frame_number, length))\n",
    "        output_movie.write(frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "input_movie.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'dtype'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-824d001eea48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcapture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     frame = display_instances(\n",
      "\u001b[1;32mC:\\Python34\\Mask_RCNN\\mrcnn\\model.py\u001b[0m in \u001b[0;36mdetect\u001b[1;34m(self, images, verbose)\u001b[0m\n\u001b[0;32m   2452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2453\u001b[0m         \u001b[1;31m# Mold inputs to format expected by the neural network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2454\u001b[1;33m         \u001b[0mmolded_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_metas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmold_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2456\u001b[0m         \u001b[1;31m# Validate image sizes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python34\\Mask_RCNN\\mrcnn\\model.py\u001b[0m in \u001b[0;36mmold_inputs\u001b[1;34m(self, images)\u001b[0m\n\u001b[0;32m   2350\u001b[0m                 \u001b[0mmin_scale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIMAGE_MIN_SCALE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2351\u001b[0m                 \u001b[0mmax_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIMAGE_MAX_DIM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2352\u001b[1;33m                 mode=self.config.IMAGE_RESIZE_MODE)\n\u001b[0m\u001b[0;32m   2353\u001b[0m             \u001b[0mmolded_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmold_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmolded_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2354\u001b[0m             \u001b[1;31m# Build image_meta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python34\\Mask_RCNN\\mrcnn\\utils.py\u001b[0m in \u001b[0;36mresize_image\u001b[1;34m(image, min_dim, max_dim, min_scale, mode)\u001b[0m\n\u001b[0;32m    423\u001b[0m     \"\"\"\n\u001b[0;32m    424\u001b[0m     \u001b[1;31m# Keep track of image dtype and return results in the same dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m     \u001b[0mimage_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m     \u001b[1;31m# Default window (y1, x1, y2, x2) and default scale == 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'dtype'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "#fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "#out = cv2.VideoWriter('output_catpure.avi',fourcc, 20.0, (640,480))\n",
    "\n",
    "    # these 2 lines can be removed if you dont have a 1080p camera.\n",
    "    #capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "    #capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "\n",
    "while True:\n",
    "    ret, frame = capture.read()\n",
    "    results = model.detect([frame], verbose=0)\n",
    "    r = results[0]\n",
    "    frame = display_instances(\n",
    "            frame, r['rois'], r['masks'], r['class_ids'], class_names, r['scores']\n",
    "        )\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "capture.release()\n",
    "#out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
